{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICf28niZMuOv",
        "outputId": "090e74aa-76c1-44b3-a880-be4485b44348"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 226MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DenseNet(\n",
            "  (features): Sequential(\n",
            "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(inplace=True)\n",
            "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (denseblock1): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition1): _Transition(\n",
            "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock2): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition2): _Transition(\n",
            "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock3): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer17): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer18): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer19): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer20): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer21): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer22): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer23): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer24): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (transition3): _Transition(\n",
            "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    )\n",
            "    (denseblock4): _DenseBlock(\n",
            "      (denselayer1): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer2): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer3): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer4): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer5): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer6): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer7): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer8): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer9): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer10): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer11): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer12): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer13): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer14): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer15): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "      (denselayer16): _DenseLayer(\n",
            "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu1): ReLU(inplace=True)\n",
            "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu2): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "\n",
            "Classifier layer: Linear(in_features=1024, out_features=1000, bias=True)\n",
            "\n",
            "Number of features in classifier: 1024\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pretrained DenseNet121\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Inspect model architecture\n",
        "print(model)\n",
        "\n",
        "# Check classifier layer\n",
        "print(\"\\nClassifier layer:\", model.classifier)\n",
        "\n",
        "# Get number of output features\n",
        "num_ftrs = model.classifier.in_features\n",
        "print(\"\\nNumber of features in classifier:\", num_ftrs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bTbY9Z7pNXea"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\"/content/drive/MyDrive/NN/dataset_split/train\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(\"/content/drive/MyDrive/NN/dataset_split/test\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6qIn8HK6Naos"
      },
      "outputs": [],
      "source": [
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(num_ftrs, 4)\n",
        ")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()          # handles logits + labels\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1aXQE3dNfMu",
        "outputId": "c23f7fe6-fefc-4348-b903-617612657d4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] Batch [0/264] Loss: 1.5445\n",
            "Epoch [1/10] Batch [50/264] Loss: 1.3672\n",
            "Epoch [1/10] Batch [100/264] Loss: 1.1518\n",
            "Epoch [1/10] Batch [150/264] Loss: 1.0334\n",
            "Epoch [1/10] Batch [200/264] Loss: 1.1254\n",
            "Epoch [1/10] Batch [250/264] Loss: 1.0108\n",
            "Epoch [1/10] Training Loss: 1.2089\n",
            "Epoch [1/10] Validation Accuracy: 67.98%\n",
            "\n",
            "Epoch [2/10] Batch [0/264] Loss: 0.9905\n",
            "Epoch [2/10] Batch [50/264] Loss: 1.0839\n",
            "Epoch [2/10] Batch [100/264] Loss: 0.8792\n",
            "Epoch [2/10] Batch [150/264] Loss: 0.8418\n",
            "Epoch [2/10] Batch [200/264] Loss: 0.9038\n",
            "Epoch [2/10] Batch [250/264] Loss: 0.8296\n",
            "Epoch [2/10] Training Loss: 0.8999\n",
            "Epoch [2/10] Validation Accuracy: 75.31%\n",
            "\n",
            "Epoch [3/10] Batch [0/264] Loss: 0.7060\n",
            "Epoch [3/10] Batch [50/264] Loss: 0.8695\n",
            "Epoch [3/10] Batch [100/264] Loss: 0.6935\n",
            "Epoch [3/10] Batch [150/264] Loss: 0.8089\n",
            "Epoch [3/10] Batch [200/264] Loss: 0.7944\n",
            "Epoch [3/10] Batch [250/264] Loss: 0.7155\n",
            "Epoch [3/10] Training Loss: 0.7589\n",
            "Epoch [3/10] Validation Accuracy: 81.36%\n",
            "\n",
            "Epoch [4/10] Batch [0/264] Loss: 0.7506\n",
            "Epoch [4/10] Batch [50/264] Loss: 0.7106\n",
            "Epoch [4/10] Batch [100/264] Loss: 0.7328\n",
            "Epoch [4/10] Batch [150/264] Loss: 0.7504\n",
            "Epoch [4/10] Batch [200/264] Loss: 0.6597\n",
            "Epoch [4/10] Batch [250/264] Loss: 0.7064\n",
            "Epoch [4/10] Training Loss: 0.6735\n",
            "Epoch [4/10] Validation Accuracy: 82.64%\n",
            "\n",
            "Epoch [5/10] Batch [0/264] Loss: 0.5214\n",
            "Epoch [5/10] Batch [50/264] Loss: 0.6987\n",
            "Epoch [5/10] Batch [100/264] Loss: 0.5404\n",
            "Epoch [5/10] Batch [150/264] Loss: 0.6962\n",
            "Epoch [5/10] Batch [200/264] Loss: 0.5778\n",
            "Epoch [5/10] Batch [250/264] Loss: 0.5947\n",
            "Epoch [5/10] Training Loss: 0.6207\n",
            "Epoch [5/10] Validation Accuracy: 83.96%\n",
            "\n",
            "Epoch [6/10] Batch [0/264] Loss: 0.5260\n",
            "Epoch [6/10] Batch [50/264] Loss: 0.5708\n",
            "Epoch [6/10] Batch [100/264] Loss: 0.6149\n",
            "Epoch [6/10] Batch [150/264] Loss: 0.6873\n",
            "Epoch [6/10] Batch [200/264] Loss: 0.6752\n",
            "Epoch [6/10] Batch [250/264] Loss: 0.4889\n",
            "Epoch [6/10] Training Loss: 0.5869\n",
            "Epoch [6/10] Validation Accuracy: 84.15%\n",
            "\n",
            "Epoch [7/10] Batch [0/264] Loss: 0.5617\n",
            "Epoch [7/10] Batch [50/264] Loss: 0.8492\n",
            "Epoch [7/10] Batch [100/264] Loss: 0.6219\n",
            "Epoch [7/10] Batch [150/264] Loss: 0.6339\n",
            "Epoch [7/10] Batch [200/264] Loss: 0.6248\n",
            "Epoch [7/10] Batch [250/264] Loss: 0.7002\n",
            "Epoch [7/10] Training Loss: 0.5645\n",
            "Epoch [7/10] Validation Accuracy: 84.53%\n",
            "\n",
            "Epoch [8/10] Batch [0/264] Loss: 0.4471\n",
            "Epoch [8/10] Batch [50/264] Loss: 0.6987\n",
            "Epoch [8/10] Batch [100/264] Loss: 0.6007\n",
            "Epoch [8/10] Batch [150/264] Loss: 0.5889\n",
            "Epoch [8/10] Batch [200/264] Loss: 0.3748\n",
            "Epoch [8/10] Batch [250/264] Loss: 0.5812\n",
            "Epoch [8/10] Training Loss: 0.5325\n",
            "Epoch [8/10] Validation Accuracy: 85.62%\n",
            "\n",
            "Epoch [9/10] Batch [0/264] Loss: 0.5775\n",
            "Epoch [9/10] Batch [50/264] Loss: 0.5541\n",
            "Epoch [9/10] Batch [100/264] Loss: 0.3669\n",
            "Epoch [9/10] Batch [150/264] Loss: 0.4417\n",
            "Epoch [9/10] Batch [200/264] Loss: 0.9117\n",
            "Epoch [9/10] Batch [250/264] Loss: 0.6387\n",
            "Epoch [9/10] Training Loss: 0.5228\n",
            "Epoch [9/10] Validation Accuracy: 85.57%\n",
            "\n",
            "Epoch [10/10] Batch [0/264] Loss: 0.5443\n",
            "Epoch [10/10] Batch [50/264] Loss: 0.4649\n",
            "Epoch [10/10] Batch [100/264] Loss: 0.6231\n",
            "Epoch [10/10] Batch [150/264] Loss: 0.5012\n",
            "Epoch [10/10] Batch [200/264] Loss: 0.5311\n",
            "Epoch [10/10] Batch [250/264] Loss: 0.4947\n",
            "Epoch [10/10] Training Loss: 0.5106\n",
            "Epoch [10/10] Validation Accuracy: 86.14%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --------------------------\n",
        "# 1. Device\n",
        "# --------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------------------------\n",
        "# 2. Transforms\n",
        "# --------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet mean\n",
        "                         [0.229, 0.224, 0.225])  # ImageNet std\n",
        "])\n",
        "\n",
        "# --------------------------\n",
        "# 3. Load dataset\n",
        "# --------------------------\n",
        "train_dataset = datasets.ImageFolder(\"/content/drive/MyDrive/NN/dataset_split/train\", transform=transform)\n",
        "test_dataset  = datasets.ImageFolder(\"/content/drive/MyDrive/NN/dataset_split/test\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------\n",
        "# 4. Load DenseNet121\n",
        "# --------------------------\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Optional: freeze all DenseNet layers first\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier\n",
        "num_ftrs = model.classifier.in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(num_ftrs, 4)  # 4 classes\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# --------------------------\n",
        "# 5. Loss and optimizer\n",
        "# --------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)  # only train classifier initially\n",
        "\n",
        "# --------------------------\n",
        "# 6. Training loop\n",
        "# --------------------------\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Print per 50 batches\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # 7. Validation per epoch\n",
        "    # --------------------------\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Accuracy: {acc:.2f}%\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 8. Optional: unfreeze top DenseNet layers for fine-tuning\n",
        "# --------------------------\n",
        "# for name, param in model.named_parameters():\n",
        "#     if \"denseblock4\" in name or \"norm5\" in name:  # unfreeze last block\n",
        "#         param.requires_grad = True\n",
        "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC_sIxhjuy3I",
        "outputId": "ae14a9fd-0b6e-41b5-a241-19b759a9b81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Initial Train] Epoch 1, Batch 0/264, Loss: 1.4135\n",
            "[Initial Train] Epoch 1, Batch 50/264, Loss: 1.1614\n",
            "[Initial Train] Epoch 1, Batch 100/264, Loss: 1.1131\n",
            "[Initial Train] Epoch 1, Batch 150/264, Loss: 1.2571\n",
            "[Initial Train] Epoch 1, Batch 200/264, Loss: 0.9676\n",
            "[Initial Train] Epoch 1, Batch 250/264, Loss: 0.9713\n",
            "[Initial Train] Epoch 1, Loss: 1.1457\n",
            "[Initial Train] Epoch 1, Validation Accuracy: 72.09%\n",
            "\n",
            "[Initial Train] Epoch 2, Batch 0/264, Loss: 0.9379\n",
            "[Initial Train] Epoch 2, Batch 50/264, Loss: 0.9193\n",
            "[Initial Train] Epoch 2, Batch 100/264, Loss: 0.9397\n",
            "[Initial Train] Epoch 2, Batch 150/264, Loss: 0.7482\n",
            "[Initial Train] Epoch 2, Batch 200/264, Loss: 0.7745\n",
            "[Initial Train] Epoch 2, Batch 250/264, Loss: 0.7142\n",
            "[Initial Train] Epoch 2, Loss: 0.8622\n",
            "[Initial Train] Epoch 2, Validation Accuracy: 79.52%\n",
            "\n",
            "[Initial Train] Epoch 3, Batch 0/264, Loss: 0.8951\n",
            "[Initial Train] Epoch 3, Batch 50/264, Loss: 0.6852\n",
            "[Initial Train] Epoch 3, Batch 100/264, Loss: 0.7673\n",
            "[Initial Train] Epoch 3, Batch 150/264, Loss: 0.7510\n",
            "[Initial Train] Epoch 3, Batch 200/264, Loss: 0.7687\n",
            "[Initial Train] Epoch 3, Batch 250/264, Loss: 0.6823\n",
            "[Initial Train] Epoch 3, Loss: 0.7324\n",
            "[Initial Train] Epoch 3, Validation Accuracy: 80.61%\n",
            "\n",
            "[Initial Train] Epoch 4, Batch 0/264, Loss: 0.7360\n",
            "[Initial Train] Epoch 4, Batch 50/264, Loss: 0.6393\n",
            "[Initial Train] Epoch 4, Batch 100/264, Loss: 0.5767\n",
            "[Initial Train] Epoch 4, Batch 150/264, Loss: 0.4895\n",
            "[Initial Train] Epoch 4, Batch 200/264, Loss: 0.6186\n",
            "[Initial Train] Epoch 4, Batch 250/264, Loss: 0.6713\n",
            "[Initial Train] Epoch 4, Loss: 0.6599\n",
            "[Initial Train] Epoch 4, Validation Accuracy: 83.35%\n",
            "\n",
            "[Initial Train] Epoch 5, Batch 0/264, Loss: 0.7006\n",
            "[Initial Train] Epoch 5, Batch 50/264, Loss: 0.5590\n",
            "[Initial Train] Epoch 5, Batch 100/264, Loss: 0.5701\n",
            "[Initial Train] Epoch 5, Batch 150/264, Loss: 0.5952\n",
            "[Initial Train] Epoch 5, Batch 200/264, Loss: 0.5056\n",
            "[Initial Train] Epoch 5, Batch 250/264, Loss: 0.6258\n",
            "[Initial Train] Epoch 5, Loss: 0.6104\n",
            "[Initial Train] Epoch 5, Validation Accuracy: 84.77%\n",
            "\n",
            "[Initial Train] Epoch 6, Batch 0/264, Loss: 0.6484\n",
            "[Initial Train] Epoch 6, Batch 50/264, Loss: 0.6120\n",
            "[Initial Train] Epoch 6, Batch 100/264, Loss: 0.5087\n",
            "[Initial Train] Epoch 6, Batch 150/264, Loss: 0.6125\n",
            "[Initial Train] Epoch 6, Batch 200/264, Loss: 0.7031\n",
            "[Initial Train] Epoch 6, Batch 250/264, Loss: 0.6109\n",
            "[Initial Train] Epoch 6, Loss: 0.5781\n",
            "[Initial Train] Epoch 6, Validation Accuracy: 85.34%\n",
            "\n",
            "[Initial Train] Epoch 7, Batch 0/264, Loss: 0.3686\n",
            "[Initial Train] Epoch 7, Batch 50/264, Loss: 0.7094\n",
            "[Initial Train] Epoch 7, Batch 100/264, Loss: 0.4384\n",
            "[Initial Train] Epoch 7, Batch 150/264, Loss: 0.3628\n",
            "[Initial Train] Epoch 7, Batch 200/264, Loss: 0.4148\n",
            "[Initial Train] Epoch 7, Batch 250/264, Loss: 0.5558\n",
            "[Initial Train] Epoch 7, Loss: 0.5532\n",
            "[Initial Train] Epoch 7, Validation Accuracy: 85.57%\n",
            "\n",
            "[Initial Train] Epoch 8, Batch 0/264, Loss: 0.4974\n",
            "[Initial Train] Epoch 8, Batch 50/264, Loss: 0.4764\n",
            "[Initial Train] Epoch 8, Batch 100/264, Loss: 0.5673\n",
            "[Initial Train] Epoch 8, Batch 150/264, Loss: 0.6633\n",
            "[Initial Train] Epoch 8, Batch 200/264, Loss: 0.4072\n",
            "[Initial Train] Epoch 8, Batch 250/264, Loss: 0.3139\n",
            "[Initial Train] Epoch 8, Loss: 0.5374\n",
            "[Initial Train] Epoch 8, Validation Accuracy: 85.53%\n",
            "\n",
            "[Initial Train] Epoch 9, Batch 0/264, Loss: 0.4162\n",
            "[Initial Train] Epoch 9, Batch 50/264, Loss: 0.4886\n",
            "[Initial Train] Epoch 9, Batch 100/264, Loss: 0.5356\n",
            "[Initial Train] Epoch 9, Batch 150/264, Loss: 0.5825\n",
            "[Initial Train] Epoch 9, Batch 200/264, Loss: 0.5226\n",
            "[Initial Train] Epoch 9, Batch 250/264, Loss: 0.6430\n",
            "[Initial Train] Epoch 9, Loss: 0.5163\n",
            "[Initial Train] Epoch 9, Validation Accuracy: 85.86%\n",
            "\n",
            "[Initial Train] Epoch 10, Batch 0/264, Loss: 0.5533\n",
            "[Initial Train] Epoch 10, Batch 50/264, Loss: 0.6290\n",
            "[Initial Train] Epoch 10, Batch 100/264, Loss: 0.5203\n",
            "[Initial Train] Epoch 10, Batch 150/264, Loss: 0.6019\n",
            "[Initial Train] Epoch 10, Batch 200/264, Loss: 0.3297\n",
            "[Initial Train] Epoch 10, Batch 250/264, Loss: 0.4724\n",
            "[Initial Train] Epoch 10, Loss: 0.4995\n",
            "[Initial Train] Epoch 10, Validation Accuracy: 85.53%\n",
            "\n",
            "[Fine-Tune] Epoch 1, Batch 0/264, Loss: 0.5582\n",
            "[Fine-Tune] Epoch 1, Batch 50/264, Loss: 0.4949\n",
            "[Fine-Tune] Epoch 1, Batch 100/264, Loss: 0.3246\n",
            "[Fine-Tune] Epoch 1, Batch 150/264, Loss: 0.3683\n",
            "[Fine-Tune] Epoch 1, Batch 200/264, Loss: 0.5407\n",
            "[Fine-Tune] Epoch 1, Batch 250/264, Loss: 0.4776\n",
            "[Fine-Tune] Epoch 1, Loss: 0.4558\n",
            "[Fine-Tune] Epoch 1, Validation Accuracy: 87.56%\n",
            "\n",
            "[Fine-Tune] Epoch 2, Batch 0/264, Loss: 0.3830\n",
            "[Fine-Tune] Epoch 2, Batch 50/264, Loss: 0.3990\n",
            "[Fine-Tune] Epoch 2, Batch 100/264, Loss: 0.5324\n",
            "[Fine-Tune] Epoch 2, Batch 150/264, Loss: 0.2998\n",
            "[Fine-Tune] Epoch 2, Batch 200/264, Loss: 0.2619\n",
            "[Fine-Tune] Epoch 2, Batch 250/264, Loss: 0.3134\n",
            "[Fine-Tune] Epoch 2, Loss: 0.3733\n",
            "[Fine-Tune] Epoch 2, Validation Accuracy: 89.50%\n",
            "\n",
            "[Fine-Tune] Epoch 3, Batch 0/264, Loss: 0.1141\n",
            "[Fine-Tune] Epoch 3, Batch 50/264, Loss: 0.3814\n",
            "[Fine-Tune] Epoch 3, Batch 100/264, Loss: 0.6438\n",
            "[Fine-Tune] Epoch 3, Batch 150/264, Loss: 0.3586\n",
            "[Fine-Tune] Epoch 3, Batch 200/264, Loss: 0.2273\n",
            "[Fine-Tune] Epoch 3, Batch 250/264, Loss: 0.1464\n",
            "[Fine-Tune] Epoch 3, Loss: 0.3269\n",
            "[Fine-Tune] Epoch 3, Validation Accuracy: 90.82%\n",
            "\n",
            "[Fine-Tune] Epoch 4, Batch 0/264, Loss: 0.3429\n",
            "[Fine-Tune] Epoch 4, Batch 50/264, Loss: 0.2510\n",
            "[Fine-Tune] Epoch 4, Batch 100/264, Loss: 0.3080\n",
            "[Fine-Tune] Epoch 4, Batch 150/264, Loss: 0.3242\n",
            "[Fine-Tune] Epoch 4, Batch 200/264, Loss: 0.3486\n",
            "[Fine-Tune] Epoch 4, Batch 250/264, Loss: 0.1734\n",
            "[Fine-Tune] Epoch 4, Loss: 0.2859\n",
            "[Fine-Tune] Epoch 4, Validation Accuracy: 91.30%\n",
            "\n",
            "[Fine-Tune] Epoch 5, Batch 0/264, Loss: 0.1264\n",
            "[Fine-Tune] Epoch 5, Batch 50/264, Loss: 0.1877\n",
            "[Fine-Tune] Epoch 5, Batch 100/264, Loss: 0.2659\n",
            "[Fine-Tune] Epoch 5, Batch 150/264, Loss: 0.3381\n",
            "[Fine-Tune] Epoch 5, Batch 200/264, Loss: 0.3676\n",
            "[Fine-Tune] Epoch 5, Batch 250/264, Loss: 0.3002\n",
            "[Fine-Tune] Epoch 5, Loss: 0.2597\n",
            "[Fine-Tune] Epoch 5, Validation Accuracy: 91.91%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --------------------------\n",
        "# 1. Device\n",
        "# --------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------------------------\n",
        "# 2. Transforms\n",
        "# --------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --------------------------\n",
        "# 3. Load dataset\n",
        "# --------------------------\n",
        "train_dataset = datasets.ImageFolder(\"/content/drive/MyDrive/NN/dataset_split/train\", transform=transform)\n",
        "test_dataset  = datasets.ImageFolder(\"/content/drive/MyDrive/NN/dataset_split/test\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --------------------------\n",
        "# 4. Load DenseNet121\n",
        "# --------------------------\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier for 4 classes with dropout\n",
        "num_ftrs = model.classifier.in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(num_ftrs, 4)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# --------------------------\n",
        "# 5. Loss and optimizer (classifier only)\n",
        "# --------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
        "\n",
        "# --------------------------\n",
        "# 6. Initial training (classifier only)\n",
        "# --------------------------\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"[Initial Train] Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"[Initial Train] Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"[Initial Train] Epoch {epoch+1}, Validation Accuracy: {acc:.2f}%\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 7. Fine-tuning last dense block\n",
        "# --------------------------\n",
        "for name, param in model.named_parameters():\n",
        "    if \"denseblock4\" in name or \"norm5\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Optimizer for fine-tuning with small learning rate\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "\n",
        "# --------------------------\n",
        "# 8. Fine-tuning training loop\n",
        "# --------------------------\n",
        "num_ft_epochs = 5\n",
        "for epoch in range(num_ft_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"[Fine-Tune] Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"[Fine-Tune] Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"[Fine-Tune] Epoch {epoch+1}, Validation Accuracy: {acc:.2f}%\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7RW5mUA18T8",
        "outputId": "9a733b5f-194d-44ce-d924-c58195a4c849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Continue FT] Epoch 1, Batch 0/264, Loss: 0.2302\n",
            "[Continue FT] Epoch 1, Batch 50/264, Loss: 0.2756\n",
            "[Continue FT] Epoch 1, Batch 100/264, Loss: 0.1377\n",
            "[Continue FT] Epoch 1, Batch 150/264, Loss: 0.2404\n",
            "[Continue FT] Epoch 1, Batch 200/264, Loss: 0.2587\n",
            "[Continue FT] Epoch 1, Batch 250/264, Loss: 0.4292\n",
            "[Continue FT] Epoch 1, Loss: 0.2363\n",
            "[Continue FT] Epoch 1, Validation Accuracy: 92.86%\n",
            "\n",
            "[Continue FT] Epoch 2, Batch 0/264, Loss: 0.0930\n",
            "[Continue FT] Epoch 2, Batch 50/264, Loss: 0.2162\n",
            "[Continue FT] Epoch 2, Batch 100/264, Loss: 0.3190\n",
            "[Continue FT] Epoch 2, Batch 150/264, Loss: 0.1743\n",
            "[Continue FT] Epoch 2, Batch 200/264, Loss: 0.7245\n",
            "[Continue FT] Epoch 2, Batch 250/264, Loss: 0.1365\n",
            "[Continue FT] Epoch 2, Loss: 0.2164\n",
            "[Continue FT] Epoch 2, Validation Accuracy: 93.71%\n",
            "\n",
            "[Continue FT] Epoch 3, Batch 0/264, Loss: 0.1418\n",
            "[Continue FT] Epoch 3, Batch 50/264, Loss: 0.1740\n",
            "[Continue FT] Epoch 3, Batch 100/264, Loss: 0.3522\n",
            "[Continue FT] Epoch 3, Batch 150/264, Loss: 0.1807\n",
            "[Continue FT] Epoch 3, Batch 200/264, Loss: 0.1300\n",
            "[Continue FT] Epoch 3, Batch 250/264, Loss: 0.3003\n",
            "[Continue FT] Epoch 3, Loss: 0.1941\n",
            "[Continue FT] Epoch 3, Validation Accuracy: 93.99%\n",
            "\n",
            "[Continue FT] Epoch 4, Batch 0/264, Loss: 0.1891\n",
            "[Continue FT] Epoch 4, Batch 50/264, Loss: 0.4190\n",
            "[Continue FT] Epoch 4, Batch 100/264, Loss: 0.2158\n",
            "[Continue FT] Epoch 4, Batch 150/264, Loss: 0.2173\n",
            "[Continue FT] Epoch 4, Batch 200/264, Loss: 0.1393\n",
            "[Continue FT] Epoch 4, Batch 250/264, Loss: 0.1121\n",
            "[Continue FT] Epoch 4, Loss: 0.1750\n",
            "[Continue FT] Epoch 4, Validation Accuracy: 94.23%\n",
            "\n",
            "[Continue FT] Epoch 5, Batch 0/264, Loss: 0.1661\n",
            "[Continue FT] Epoch 5, Batch 50/264, Loss: 0.1538\n",
            "[Continue FT] Epoch 5, Batch 100/264, Loss: 0.1420\n",
            "[Continue FT] Epoch 5, Batch 150/264, Loss: 0.1315\n",
            "[Continue FT] Epoch 5, Batch 200/264, Loss: 0.1493\n",
            "[Continue FT] Epoch 5, Batch 250/264, Loss: 0.0494\n",
            "[Continue FT] Epoch 5, Loss: 0.1617\n",
            "[Continue FT] Epoch 5, Validation Accuracy: 94.75%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --------------------------\n",
        "# Continue fine-tuning\n",
        "# --------------------------\n",
        "num_more_epochs = 5  # or 3\n",
        "model.train()  # make sure model is in training mode\n",
        "\n",
        "# Optimizer remains the same (for last dense block + classifier)\n",
        "# If you restarted, re-create optimizer like this:\n",
        "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "\n",
        "for epoch in range(num_more_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"[Continue FT] Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"[Continue FT] Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Validation accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"[Continue FT] Epoch {epoch+1}, Validation Accuracy: {acc:.2f}%\\n\")\n",
        "    model.train()  # back to training mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udZVl3F24W8N",
        "outputId": "7b2bb7f2-135a-4a44-e81b-5ba27ba142b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Continue FT] Epoch 1, Batch 0/264, Loss: 0.1777\n",
            "[Continue FT] Epoch 1, Batch 50/264, Loss: 0.0506\n",
            "[Continue FT] Epoch 1, Batch 100/264, Loss: 0.3216\n",
            "[Continue FT] Epoch 1, Batch 150/264, Loss: 0.2303\n",
            "[Continue FT] Epoch 1, Batch 200/264, Loss: 0.1147\n",
            "[Continue FT] Epoch 1, Batch 250/264, Loss: 0.0882\n",
            "[Continue FT] Epoch 1, Loss: 0.1423\n",
            "[Continue FT] Epoch 1, Validation Accuracy: 95.22%\n",
            "\n",
            "[Continue FT] Epoch 2, Batch 0/264, Loss: 0.2080\n",
            "[Continue FT] Epoch 2, Batch 50/264, Loss: 0.2356\n",
            "[Continue FT] Epoch 2, Batch 100/264, Loss: 0.1724\n",
            "[Continue FT] Epoch 2, Batch 150/264, Loss: 0.1624\n",
            "[Continue FT] Epoch 2, Batch 200/264, Loss: 0.1230\n",
            "[Continue FT] Epoch 2, Batch 250/264, Loss: 0.2286\n",
            "[Continue FT] Epoch 2, Loss: 0.1313\n",
            "[Continue FT] Epoch 2, Validation Accuracy: 95.41%\n",
            "\n",
            "[Continue FT] Epoch 3, Batch 0/264, Loss: 0.0352\n",
            "[Continue FT] Epoch 3, Batch 50/264, Loss: 0.1027\n",
            "[Continue FT] Epoch 3, Batch 100/264, Loss: 0.1196\n",
            "[Continue FT] Epoch 3, Batch 150/264, Loss: 0.3032\n",
            "[Continue FT] Epoch 3, Batch 200/264, Loss: 0.0826\n",
            "[Continue FT] Epoch 3, Batch 250/264, Loss: 0.2226\n",
            "[Continue FT] Epoch 3, Loss: 0.1232\n",
            "[Continue FT] Epoch 3, Validation Accuracy: 95.74%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --------------------------\n",
        "# Continue fine-tuning\n",
        "# --------------------------\n",
        "num_more_epochs = 3  # or 3\n",
        "model.train()  # make sure model is in training mode\n",
        "\n",
        "# Optimizer remains the same (for last dense block + classifier)\n",
        "# If you restarted, re-create optimizer like this:\n",
        "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "\n",
        "for epoch in range(num_more_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"[Continue FT] Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"[Continue FT] Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Validation accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"[Continue FT] Epoch {epoch+1}, Validation Accuracy: {acc:.2f}%\\n\")\n",
        "    model.train()  # back to training mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_YaeOFm2LPZ",
        "outputId": "5b69bde5-eaea-4e23-9a1e-24286aa16ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights saved to: /content/drive/MyDrive/NN/densenet_mri_ft.pth\n"
          ]
        }
      ],
      "source": [
        "# Save the model weights\n",
        "save_path = '/content/drive/MyDrive/NN/densenet_mri_ft.pth'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model weights saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzh4cm0D2_vn",
        "outputId": "48933a27-ead9-448f-bf22-3dcfb1a0fba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss for final model: 0.1369\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # set model to evaluation mode\n",
        "val_loss = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)  # CrossEntropyLoss\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "val_loss /= total_samples\n",
        "print(f\"Validation Loss for final model: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDXcb-tQ2OQG"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(save_path))\n",
        "model.to(device)\n",
        "model.eval()  # for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsRPg9kI2Zbb",
        "outputId": "1c09bbd8-60f8-4b64-b7a9-c5ffc49fa90e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-class Precision: [0.98340249 0.90644491 0.96111111 0.96545455]\n",
            "Per-class Recall: [0.94673768 0.92963753 0.98295455 0.9797048 ]\n",
            "Per-class F1-score: [0.96472185 0.91789474 0.97191011 0.97252747]\n",
            "Confusion Matrix:\n",
            " [[711  30   6   4]\n",
            " [ 10 436   8  15]\n",
            " [  1   5 346   0]\n",
            " [  1  10   0 531]]\n",
            "Macro Precision: 0.9541, Macro Recall: 0.9598, Macro F1: 0.9568\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_preds, average=None)  # per class\n",
        "recall = recall_score(all_labels, all_preds, average=None)\n",
        "f1 = f1_score(all_labels, all_preds, average=None)\n",
        "conf_mat = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "print(\"Per-class Precision:\", precision)\n",
        "print(\"Per-class Recall:\", recall)\n",
        "print(\"Per-class F1-score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_mat)\n",
        "\n",
        "# Optional: macro/micro averages\n",
        "precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
        "recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
        "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "print(f\"Macro Precision: {precision_macro:.4f}, Macro Recall: {recall_macro:.4f}, Macro F1: {f1_macro:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
